{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SQuAD_Training.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNcSgmS9aF3sf/qeYB5gGTn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"i4612_j6XU8p","colab_type":"code","outputId":"0e3ee085-465e-44fc-f6e3-20c37016c788","executionInfo":{"status":"ok","timestamp":1579803805614,"user_tz":300,"elapsed":7719,"user":{"displayName":"Michael Gunn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDLPGDDbbM21TTQ016rwUebXbiYn0V96tePqWjuFA=s64","userId":"01288302193981142685"}},"colab":{"base_uri":"https://localhost:8080/","height":382}},"source":["!pip install transformers\n","!pip install wget"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.3.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n","Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (2.6.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n","Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"njT_xpn7XZuG","colab_type":"code","outputId":"3c373d1e-2479-47f2-f1f7-a6e6a5fd5641","executionInfo":{"status":"ok","timestamp":1579803810149,"user_tz":300,"elapsed":12249,"user":{"displayName":"Michael Gunn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDLPGDDbbM21TTQ016rwUebXbiYn0V96tePqWjuFA=s64","userId":"01288302193981142685"}},"colab":{"base_uri":"https://localhost:8080/","height":64}},"source":["import wget\n","import os\n","import json\n","import torch\n","import numpy as np\n","import pandas as pd\n","from transformers import BertModel, BertTokenizer, BertForQuestionAnswering\n","#from torch.utils.data import Dataset, DataLoader"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"HChdOtKopEGH","colab_type":"code","colab":{}},"source":["def SQuAD_organize(SQuAD):\n","\n","\t\"\"\"\n","\ttakes the SQuAD json file and organizes the important parts into a list of lists\n","\timpossible questions are \"answered\" with an empty string\n","\t\"\"\"\n","\n","\tSQuAD_list_of_lists = []\n","\n","\tfor i, subject in enumerate(SQuAD['data']):\t\t\t\t\t\t\t\t\t\t\t\t\t#limited for testing purposes\n","\t\tfor paragraph in SQuAD['data'][i]['paragraphs'][0:2]:\t\t\t\t\t\t\t\t\t#limited for testing purposes\n","\t\t\t#find the context (paragraph)\n","\t\t\tthe_context = paragraph['context']\n","\t\t\tfor j, questions in enumerate(SQuAD['data'][i]['paragraphs']):\t#limited for testing purposes\n","\t\t\t\tfor question in questions['qas'][0:2]:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#limited for testing purposes\n","\t\t\t\t\t#find the question\n","\t\t\t\t\tthe_question = question['question']\n","\t\t\t\t\tif question['answers'] != []:\n","\t\t\t\t\t\t#find the answer (label)\n","\t\t\t\t\t\tthe_answer = question['answers'][0]['text']\n","\t\t\t\t\telse:\n","\t\t\t\t\t\t#or an empty string for impossible questions\n","\t\t\t\t\t\tthe_answer = ''\n","\t\t\t\t\trow = [the_context.lower(), the_question.lower(), the_answer.lower()]\n","\t\t\t\t\tSQuAD_list_of_lists.append(row)\n","\n","\tSQuAD_df = pd.DataFrame(SQuAD_list_of_lists, columns=['context', 'question', 'answer'])\n","\n","\treturn SQuAD_df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yT9gHIbfXg8U","colab_type":"code","outputId":"3ef2fee3-4ace-4b2e-8915-20baa4a27b80","executionInfo":{"status":"ok","timestamp":1579803814673,"user_tz":300,"elapsed":16759,"user":{"displayName":"Michael Gunn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDLPGDDbbM21TTQ016rwUebXbiYn0V96tePqWjuFA=s64","userId":"01288302193981142685"}},"colab":{"base_uri":"https://localhost:8080/","height":206}},"source":["#download the dataset from the github repository of the webinar\n","url_train = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json'\n","url_dev = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json'\n","if not os.path.exists('./train-v2.0.json'):\n","  wget.download(url_train, './train-v2.0.json')\n","if not os.path.exists('./dev-v2.0.json'):\n","  wget.download(url_dev, './dev-v2.0.json')\n","\n","with open('train-v2.0.json', 'r') as json_train:\n","\tSQuAD_train = json.load(json_train, encoding='utf-8')\n","with open('dev-v2.0.json', 'r') as json_val:\n","\tSQuAD_val = json.load(json_val, encoding='utf-8')\n"," \n","SQuAD_train_df = SQuAD_organize(SQuAD_train)\n","SQuAD_val_df = SQuAD_organize(SQuAD_val)\n","SQuAD_train_df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>context</th>\n","      <th>question</th>\n","      <th>answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>beyoncé giselle knowles-carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>when did beyonce start becoming popular?</td>\n","      <td>in the late 1990s</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>beyoncé giselle knowles-carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>what areas did beyonce compete in when she was...</td>\n","      <td>singing and dancing</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>beyoncé giselle knowles-carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>after her second solo album, what other entert...</td>\n","      <td>acting</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>beyoncé giselle knowles-carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>which artist did beyonce marry?</td>\n","      <td>jay z</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>beyoncé giselle knowles-carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>in her music, what are some recurring elements...</td>\n","      <td>love, relationships, and monogamy</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             context  ...                             answer\n","0  beyoncé giselle knowles-carter (/biːˈjɒnseɪ/ b...  ...                  in the late 1990s\n","1  beyoncé giselle knowles-carter (/biːˈjɒnseɪ/ b...  ...                singing and dancing\n","2  beyoncé giselle knowles-carter (/biːˈjɒnseɪ/ b...  ...                             acting\n","3  beyoncé giselle knowles-carter (/biːˈjɒnseɪ/ b...  ...                              jay z\n","4  beyoncé giselle knowles-carter (/biːˈjɒnseɪ/ b...  ...  love, relationships, and monogamy\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"zgs4K0OPY5pp","colab_type":"code","colab":{}},"source":["#customize tokenizer (this is just to confirm that it's possible)\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","#extra_tokens = ['digital', 'virtual', 'blockchain', 'internet', 'software'] #this is just until we decide what the real list will be\n","#tokenizer.add_tokens(extra_tokens)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lo7se_yuhOPx","colab_type":"code","colab":{}},"source":["from torch.utils.data import Dataset, DataLoader"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PKb7LtFlYtR2","colab_type":"code","colab":{}},"source":["class LoadDataset(Dataset):\n","\n","  def __init__(self, df, tokenizer, maxlen=128):\n","    self.df = df\n","    self.tokenizer = tokenizer\n","    self.maxlen = maxlen\n","\n","  #dataset class is required to return the max length of any given string\n","  def __len__(self):\n","    return len(self.df)\n","\n","  #preprocessing and returning weights by using the data loader\n","  def __getitem__(self, index):\n","    context = self.df.loc[index, 'context']\n","    question = self.df.loc[index, 'question']\n","    label = self.df.loc[index, 'answer']\n","\n","    #tokenization, special tokens, padding, attention mask, token ids\n","    tokens_q = self.tokenizer.tokenize(question)                          #tokenization\n","    tokens_q = ['[CLS]'] + tokens_q + ['[SEP]']                           #special tokens\n","    tokens_c = self.tokenizer.tokenize(context)\n","    tokens_c = tokens_c + ['[SEP]']\n","    tokens = tokens_q + tokens_c\n","    if len(tokens) < self.maxlen:                                         #padding\n","      tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))]\n","    else:\n","      tokens = tokens[:self.maxlen-1] + ['[SEP]']\n","    token_ids = self.tokenizer.convert_tokens_to_ids(tokens)              #token ids\n","    token_ids = torch.tensor(token_ids)\n","    \n","    attn_masks = (token_ids != 0).long()                                   #attention mask (if token id is not 0, return true as integer (1))\n"," \n","    segment_ids = [0 for _ in range(len(tokens_q))] + [1 for _ in range(self.maxlen-len(tokens_q))]\n","    segment_ids = torch.tensor(segment_ids)                               #segment ids (to distinguish the first passage from the second)\n","\n","\n","    return token_ids, attn_masks, segment_ids, label #this would also include the position tag if a next-sentence model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PZCDe-TgqVV8","colab_type":"code","outputId":"8a924800-7acd-4910-d052-4bc727cd06c0","executionInfo":{"status":"ok","timestamp":1579803817659,"user_tz":300,"elapsed":19724,"user":{"displayName":"Michael Gunn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDLPGDDbbM21TTQ016rwUebXbiYn0V96tePqWjuFA=s64","userId":"01288302193981142685"}},"colab":{"base_uri":"https://localhost:8080/","height":243}},"source":["train_set = LoadDataset(df=SQuAD_train_df, tokenizer=tokenizer, maxlen=128)\n","val_set = LoadDataset(df=SQuAD_val_df, tokenizer=tokenizer, maxlen=128)\n","print(train_set[50][0].shape, train_set[50][1].shape, train_set[50][2].shape, train_set[50][0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([128]) torch.Size([128]) torch.Size([128]) tensor([  101, 20773,  2207,  1996,  2299,  1000,  4195,  1000,  2006,  2029,\n","         3784,  2189,  2326,  1029,   102, 20773, 21025, 19358, 22815,  1011,\n","         5708,  1006,  1013, 12170, 23432, 29715,  3501, 29678, 12325, 29685,\n","         1013, 10506,  1011, 10930,  2078,  1011,  2360,  1007,  1006,  2141,\n","         2244,  1018,  1010,  3261,  1007,  2003,  2019,  2137,  3220,  1010,\n","         6009,  1010,  2501,  3135,  1998,  3883,  1012,  2141,  1998,  2992,\n","         1999,  5395,  1010,  3146,  1010,  2016,  2864,  1999,  2536,  4823,\n","         1998,  5613,  6479,  2004,  1037,  2775,  1010,  1998,  3123,  2000,\n","         4476,  1999,  1996,  2397,  4134,  2004,  2599,  3220,  1997,  1054,\n","         1004,  1038,  2611,  1011,  2177, 10461,  1005,  1055,  2775,  1012,\n","         3266,  2011,  2014,  2269,  1010, 25436, 22815,  1010,  1996,  2177,\n","         2150,  2028,  1997,  1996,  2088,  1005,  1055,  2190,  1011,  4855,\n","         2611,  2967,  1997,  2035,  2051,  1012,  2037,   102])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M3EQRNits1NY","colab_type":"code","colab":{}},"source":["train_loader = DataLoader(train_set, batch_size=32, num_workers=5)\n","val_loader = DataLoader(val_set, batch_size=32, num_workers=5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yXNXgun9Rjt6","colab_type":"code","colab":{}},"source":["model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","#model.resize_token_embeddings(len(tokenizer))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s_HCcd8iURYv","colab_type":"code","outputId":"c73264ef-f0f1-4af8-daa4-17e9f24a76dc","executionInfo":{"status":"ok","timestamp":1579803931574,"user_tz":300,"elapsed":133622,"user":{"displayName":"Michael Gunn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDLPGDDbbM21TTQ016rwUebXbiYn0V96tePqWjuFA=s64","userId":"01288302193981142685"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["if torch.cuda.is_available():\n","  device = 'cuda'\n","  print(\"Using the GPU: \" + torch.cuda.get_device_name(0))\n","else:\n","  device = 'cpu'\n","  print(\"No GPU available. Using CPU instead.\")\n","print(device)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using the GPU: Tesla P100-PCIE-16GB\n","cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xy3thVHVaA3k","colab_type":"code","outputId":"a4ef444c-2fca-477a-bdc1-8f7a84f19ac9","executionInfo":{"status":"ok","timestamp":1579803937333,"user_tz":300,"elapsed":139373,"user":{"displayName":"Michael Gunn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDLPGDDbbM21TTQ016rwUebXbiYn0V96tePqWjuFA=s64","userId":"01288302193981142685"}},"colab":{"base_uri":"https://localhost:8080/","height":225}},"source":["#confirm that the GPU memory is available (this must be done or else I get a \"RuntimeError: CUDA out of memory.\" message and the model doesn't train)\n","!pip install gputil\n","import GPUtil\n","GPUtil.showUtilization()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting gputil\n","  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n","Building wheels for collected packages: gputil\n","  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7410 sha256=4eb5789a7be2bec10862295fa26acd73ec66c962230b2f165cd1d71d62c77306\n","  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n","Successfully built gputil\n","Installing collected packages: gputil\n","Successfully installed gputil-1.4.0\n","| ID | GPU | MEM |\n","------------------\n","|  0 |  0% |  0% |\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jGX6nJpJS5MX","colab_type":"code","outputId":"20601c67-cc37-4ff9-d1f6-68c06fdf51eb","executionInfo":{"status":"ok","timestamp":1579803938754,"user_tz":300,"elapsed":140787,"user":{"displayName":"Michael Gunn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDLPGDDbbM21TTQ016rwUebXbiYn0V96tePqWjuFA=s64","userId":"01288302193981142685"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["eg_question = \"Does Walmart need a software development project?\"\n","eg_context = \"\"\"\n","  Retailer Walmart China launched its first Blockchain Traceability Platform on June 25, allowing consumers to acquire more knowledge on products -- from sources, logistics and testing results -- and empowering suppliers to better safeguard food safety.\n","  The first batch of 23 products underwent testing and was introduced into the platform. The company will scale up to more than 100 merchandises by the second half of this year, covering over 10 categories, including fresh meat, vegetable, seafood and its self-developed private brands.\n","  By the end of 2020, Walmart China's traceability system will be able to track about half of the total packaged fresh meat, 40 percent of packaged vegetables and 12.5 percent of seafood.\n","  The blockchain traceability system was developed by PwC and VeChain. By scanning the products, consumers can acquire large amount of information, which cannot be tampered with once the data are written.\n","\"\"\"\n","token_ids = tokenizer.encode(eg_question, eg_context)\n","attn_masks = [0 if i <= token_ids.index(102) else 1 for i in range(len(token_ids))]\n","print(token_ids, attn_masks)\n","start_scores, end_scores = model(torch.tensor([token_ids]), token_type_ids=torch.tensor([attn_masks]))\n","#print(start_scores.shape, end_scores.shape)\n","all_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n","print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[101, 2515, 24547, 22345, 2342, 1037, 4007, 2458, 2622, 1029, 102, 20196, 24547, 22345, 2859, 3390, 2049, 2034, 3796, 24925, 2078, 7637, 8010, 4132, 2006, 2238, 2423, 1010, 4352, 10390, 2000, 9878, 2062, 3716, 2006, 3688, 1011, 1011, 2013, 4216, 1010, 12708, 1998, 5604, 3463, 1011, 1011, 1998, 7861, 23948, 20141, 2000, 2488, 28805, 2833, 3808, 1012, 1996, 2034, 14108, 1997, 2603, 3688, 9601, 5604, 1998, 2001, 3107, 2046, 1996, 4132, 1012, 1996, 2194, 2097, 4094, 2039, 2000, 2062, 2084, 2531, 16359, 2015, 2011, 1996, 2117, 2431, 1997, 2023, 2095, 1010, 5266, 2058, 2184, 7236, 1010, 2164, 4840, 6240, 1010, 15415, 1010, 23621, 1998, 2049, 2969, 1011, 2764, 2797, 9639, 1012, 2011, 1996, 2203, 1997, 12609, 1010, 24547, 22345, 2859, 1005, 1055, 7637, 8010, 2291, 2097, 2022, 2583, 2000, 2650, 2055, 2431, 1997, 1996, 2561, 21972, 4840, 6240, 1010, 2871, 3867, 1997, 21972, 11546, 1998, 2260, 1012, 1019, 3867, 1997, 23621, 1012, 1996, 3796, 24925, 2078, 7637, 8010, 2291, 2001, 2764, 2011, 1052, 16526, 1998, 2310, 24925, 2078, 1012, 2011, 13722, 1996, 3688, 1010, 10390, 2064, 9878, 2312, 3815, 1997, 2592, 1010, 2029, 3685, 2022, 17214, 4842, 2098, 2007, 2320, 1996, 2951, 2024, 2517, 1012, 102] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","the company will scale up to more than 100 merchandise ##s by the second half of this year\n"],"name":"stdout"}]}]}